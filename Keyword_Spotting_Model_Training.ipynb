{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1tV6y8GUO8v2AFFGxytQg5mk0AL-a1Ai7",
      "authorship_tag": "ABX9TyNiiMppyV0TNzNZXuKcTDDm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TevinMusau/Integrating_Voice_to_Mobile_Payment_Systems_Using_Convolutional_Neural_Networks-A_Case_of_MPESA/blob/model/Keyword_Spotting_Model_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "YXOj0oW7hZxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting Google Drive to obtain the data and store it\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to the preprocessed JSON file data\n",
        "DATA_PATH = \"/content/drive/MyDrive/ICS_PROJECT/Modules/Keyword_Spotting/Outputs/prepared_data.json\"\n",
        "\n",
        "# constant to be used in model optimization\n",
        "LEARNING_RATE = 0.0001\n",
        "\n",
        "# Number of Iterations\n",
        "EPOCHS = 40\n",
        "\n",
        "# Number of samples to consider per epoch\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Path the where the model will be saved\n",
        "SAVED_MODEL_PATH = \"/content/drive/MyDrive/ICS_PROJECT/Modules/Keyword_Spotting/Outputs/model.h5\"\n",
        "\n",
        "# Number of neurons at output layer\n",
        "NUM_KEYWORDS = 12"
      ],
      "metadata": {
        "id": "OVkyJunWWOXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(data_path):\n",
        "\n",
        "  # open the path in \"read mode\" and read the dataset\n",
        "  with open(data_path, \"r\") as fp:\n",
        "    data = json.load(fp)\n",
        "  \n",
        "  # extract the inputs (X) and outputs (y) from the prepared_data.json file\n",
        "    # Recall: the data was stored in a dictionary\n",
        "    # convert the lists to numpy arrays\n",
        "  X = np.array(data[\"MFCCs\"])\n",
        "  y = np.array(data[\"labels\"])\n",
        "\n",
        "  return X, y\n",
        "\n",
        "def get_data_splits(data_path, test_size = 0.1, test_validation = 0.1):\n",
        "\n",
        "  # load the dataset\n",
        "  X, y = load_dataset(data_path)\n",
        "\n",
        "  # create the train/ validation/ test splits\n",
        "    # these will be 2D arrays as per the prepared_data.json created\n",
        "    # test_size = proportion of the dataset for testing/ validation (10% of dataset)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size)\n",
        "  X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size = test_validation)\n",
        "  \n",
        "  # convert inputs from 2D to 3D arrays\n",
        "  X_train = X_train[..., np.newaxis]\n",
        "  # X_validation = X_train[..., np.newaxis]\n",
        "  # X_test = X_train[..., np.newaxis]\n",
        "  # print(X_train.shape, X_test.shape)\n",
        "  # print(y_train.shape, y_test.shape)\n",
        "\n",
        "  return X_train, X_validation, X_test, y_train, y_validation, y_test\n"
      ],
      "metadata": {
        "id": "CU1VngrUgTFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pbZziPoVHl-"
      },
      "outputs": [],
      "source": [
        "# load train/validation/test data splits\n",
        "\n",
        "X_train, X_validation, X_test, y_train, y_validation, y_test = get_data_splits(DATA_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(input_shape, learning_rate, error = \"sparse_categorical_crossentropy\"):\n",
        "\n",
        "  # build the network\n",
        "    # create a sequential network\n",
        "  model = keras.Sequential()\n",
        "\n",
        "    # 3 Convolutional Layers\n",
        "      # conv layer 1\n",
        "        # Params\n",
        "          # 64 = number of filters\n",
        "          # (3, 3) = Kernal size\n",
        "          # activation = activation function for non-linearity (set to ReLu)\n",
        "          # input_shape = shape/ order of input data\n",
        "          # kernel_regularizer = prevents overfitting (using l2 regularization)\n",
        "  model.add(keras.layers.Conv2D(64, (3, 3), activation=\"relu\", input_shape = input_shape, kernel_regularizer = keras.regularizers.l2(0.001)))\n",
        "        \n",
        "        # another layer that does batch normalization\n",
        "  model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "        # another layer for Max Pooling\n",
        "  model.add(keras.layers.MaxPool2D((3, 3), strides = (2, 2), padding = \"same\"))\n",
        "\n",
        "      # conv layer 2\n",
        "      # Params\n",
        "          # 32 = number of filters\n",
        "          # (3, 3) = Kernal size\n",
        "          # activation = activation function for non-linearity (set to ReLu)\n",
        "          # input_shape = shape/ order of input data\n",
        "          # kernel_regularizer = prevents overfitting (using l2 regularization)\n",
        "  model.add(keras.layers.Conv2D(32, (3, 3), activation=\"relu\", kernel_regularizer = keras.regularizers.l2(0.001)))\n",
        "        \n",
        "        # another layer that does batch normalization\n",
        "  model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "        # another layer for Max Pooling\n",
        "  model.add(keras.layers.MaxPool2D((3, 3), strides = (2, 2), padding = \"same\"))\n",
        "\n",
        "      # conv layer 3\n",
        "      # Params\n",
        "          # 32 = number of filters\n",
        "          # (2, 2) = Kernal size\n",
        "          # activation = activation function for non-linearity (set to ReLu)\n",
        "          # input_shape = shape/ order of input data\n",
        "          # kernel_regularizer = prevents overfitting (using l2 regularization)\n",
        "  model.add(keras.layers.Conv2D(32, (2, 2), activation=\"relu\", kernel_regularizer = keras.regularizers.l2(0.001)))\n",
        "        \n",
        "        # another layer that does batch normalization\n",
        "  model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "        # another layer for Max Pooling\n",
        "  model.add(keras.layers.MaxPool2D((2, 2), strides = (2, 2), padding = \"same\"))\n",
        "\n",
        "    # flatten the output to a 1D array and feed it into a dense layer\n",
        "  model.add(keras.layers.Flatten())\n",
        "\n",
        "      # the Dense Layer (fully connected)\n",
        "        # Params\n",
        "          # 64 = number of neurons\n",
        "          # activation = activation function for non-linearity (set to ReLu)\n",
        "  model.add(keras.layers.Dense(64, activation = \"relu\"))\n",
        "\n",
        "      # adding dropout layer to reduce overfitting (30% of the neurons)\n",
        "  model.add(keras.layers.Dropout(0.3))\n",
        "\n",
        "    # softmax classifer (output layer)\n",
        "  model.add(keras.layers.Dense(NUM_KEYWORDS, activation = \"softmax\"))\n",
        "\n",
        "  # compile the model\n",
        "    # using the ADAM optimizer\n",
        "  optimizer = keras.optimizers.Adam(learning_rate = learning_rate)\n",
        "\n",
        "    # Params\n",
        "      # loss -> the loss algo used to minimise error (set to sparse categorical cross entropy loss function)\n",
        "      # metrics -> what we track during training\n",
        "  model.compile(optimizer = optimizer, loss = error, metrics = [\"accuracy\"])\n",
        "\n",
        "  # print model overview\n",
        "  model.summary()\n",
        "\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "VFynGk8Cl13B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build the CNN Model\n",
        "\n",
        "# X_train is an N-D Array of input data (3D Input)\n",
        "  # X_train.shape[1] = shape for first dimension -> Number of segments (length of segment = total_samples/ hop_length)\n",
        "  # X_train.shape[2] = shape for second dimension -> Number of co-effients (MFCC co-efficients = 13)\n",
        "  # X_train.shape[3] = shape for third dimension -> Carries information about the depth (Set to 1)\n",
        "input_shape = (X_train.shape[1], X_train.shape[2], X_train.shape[3])\n",
        "\n",
        "# Params\n",
        "  # input_shape = shape/ order of the input data being fed to the CNN model\n",
        "  # LEARNING_RATE = constant to be used in model optimization\n",
        "model = build_model(input_shape, LEARNING_RATE)"
      ],
      "metadata": {
        "id": "OgnWED5IXPoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "\n",
        "# Params\n",
        "  # X_train = inputs\n",
        "  # y_train = outputs\n",
        "  # epochs = iterations\n",
        "  # batch_size = number of data samples to consider per epoch\n",
        "  # validation_data = has inputs and outputs for model optimization\n",
        "model.fit(X_train, y_train, epochs = EPOCHS, batch_size = BATCH_SIZE, validation_data = (X_validation, y_validation))"
      ],
      "metadata": {
        "id": "__fEt7-sdFUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the model on the test set\n",
        "\n",
        "test_error, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Error: {test_error}, Test Accuracy: {test_accuracy}\")"
      ],
      "metadata": {
        "id": "M4xmyCPNehgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model\n",
        "\n",
        "model.save(SAVED_MODEL_PATH)"
      ],
      "metadata": {
        "id": "OfvQOeh8fLCq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}